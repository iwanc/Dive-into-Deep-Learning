<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

## 1çº¿æ€§å›å½’
### 1.1ä¾‹å­
  $$price = w_{area}.area + w_{age}.age +b$$
  è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æˆ¿ä»·é¢„æµ‹ï¼Œæˆ¿ä»·å’Œæˆ¿å­çš„é¢ç§¯å’Œå¹´é¾„æœ‰çº¿æ€§å…³ç³»ã€‚

é€šç”¨çš„çº¿æ€§å›å½’æ¨¡å‹æˆ‘ä»¬ä½¿ç”¨$y = w\times x+b$æ¥è¡¨ç¤º
### 1.2æŸå¤±å‡½æ•°
å¹³æ–¹æŸå¤±å‡½æ•°$l_i(w,b) = \frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2$

### 1.3éšæœºæ¢¯åº¦ä¸‹é™
åœ¨æ±‚æ•°å€¼è§£çš„ä¼˜åŒ–ç®—æ³•ä¸­ï¼Œå°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆmini-batch stochastic gradient descentï¼‰åœ¨æ·±åº¦å­¦ä¹ ä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚å®ƒçš„ç®—æ³•å¾ˆç®€å•ï¼šå…ˆé€‰å–ä¸€ç»„æ¨¡å‹å‚æ•°çš„åˆå§‹å€¼ï¼Œå¦‚éšæœºé€‰å–ï¼›æ¥ä¸‹æ¥å¯¹å‚æ•°è¿›è¡Œå¤šæ¬¡è¿­ä»£ï¼Œä½¿æ¯æ¬¡è¿­ä»£éƒ½å¯èƒ½é™ä½æŸå¤±å‡½æ•°çš„å€¼ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå…ˆéšæœºå‡åŒ€é‡‡æ ·ä¸€ä¸ªç”±å›ºå®šæ•°ç›®è®­ç»ƒæ•°æ®æ ·æœ¬æ‰€ç»„æˆçš„å°æ‰¹é‡ï¼ˆmini-batchï¼‰ï¼Œç„¶åæ±‚å°æ‰¹é‡ä¸­æ•°æ®æ ·æœ¬çš„å¹³å‡æŸå¤±æœ‰å…³æ¨¡å‹å‚æ•°çš„å¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰ï¼Œæœ€åç”¨æ­¤ç»“æœä¸é¢„å…ˆè®¾å®šçš„ä¸€ä¸ªæ­£æ•°çš„ä¹˜ç§¯ä½œä¸ºæ¨¡å‹å‚æ•°åœ¨æœ¬æ¬¡è¿­ä»£çš„å‡å°é‡ã€‚

$$(w,b) \leftarrow(w,b) - \frac{\eta}{|B|}\sum_{i\in B}\delta_{(w,b)}l^{(i)}(w,b)	$$

#### éšæœºæ¢¯åº¦ä¸‹é™ä»0å¼€å§‹ä»£ç ä¾‹å­åˆ†æ
éšæœºæ¢¯åº¦ä¸‹é™å‡½æ•°è‡ªå®šä¹‰
```python
def sgd(params, lr, batch_size): 
    for param in params:
        param.data -= lr * param.grad / batch_size
```
å…¶ä¸­lræ˜¯æˆ‘ä»¬è®­ç»ƒä¸­è®¾ç½®çš„ç³»æ•°å†³å®šæ¯æ¬¡æ­¥é•¿çš„å¤§å°ã€‚ç›¸å½“äºæˆ‘ä»¬å‰é¢å…¬å¼ä¸­çš„$\etaå‚æ•°$

```python
or epoch in range(num_epochs):  # training repeats num_epochs times
    # in each epoch, all the samples in dataset will be used once
    
    # X is the feature and y is the label of a batch sample
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y).sum()  
        # calculate the gradient of batch sample loss 
        l.backward()  
        # using small batch random gradient descent to iter model parameters
        sgd([w, b], lr, batch_size)  
        # reset parameter gradient
        w.grad.data.zero_()
        b.grad.data.zero_()
    train_l = loss(net(features, w, b), labels)
    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))
```
ä»£ç ä¸­åº”ç”¨äº†.gradæ–¹æ³•è¯¥æ–¹æ³•èƒ½å¤Ÿåå‘æ±‚å‡ºæ¢¯åº¦ï¼Œæ–¹ä¾¿æˆ‘ä»¬ç†è§£éšæœºæ¢¯åº¦ä¸‹é™çš„åŸç†ã€‚

é¡ºä¾¿è€¶è®°å½•ä¸€ä¸‹pytorchä¸­å·²ç»å†…ç½®çš„çº¿æ€§å›å½’ä»£ç 
```python
class LinearNet(nn.Module):
    def __init__(self, n_feature):
        super(LinearNet, self).__init__()      # call father function to init 
        self.linear = nn.Linear(n_feature, 1)  # function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`

    def forward(self, x):
        y = self.linear(x)
        return y
    
net = LinearNet(num_inputs)
print(net)
```

### 1.4è¯¾åé”™é¢˜åˆ†æ
å‡å¦‚ä½ æ­£åœ¨å®ç°ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œå…¨è¿æ¥å±‚çš„è¾“å…¥å½¢çŠ¶æ˜¯$7 \times 8$ ï¼Œè¾“å‡ºå½¢çŠ¶æ˜¯$7 \times 1$ï¼Œå…¶ä¸­7æ˜¯æ‰¹é‡å¤§å°ï¼Œåˆ™æƒé‡å‚æ•°wwå’Œåç½®å‚æ•°bbçš„å½¢çŠ¶åˆ†åˆ«æ˜¯____å’Œ____

ç­”æ¡ˆï¼š$8\times 1$ï¼Œ$1 \times 1$
bæ˜¯æ˜¯ä¸€ä¸ªå¸¸æ•°æ¯ä¸ªå®ä¾‹éƒ½æ˜¯ä¸€æ ·çš„æ‰€ä»¥æ˜¯$1\times 1$çš„ã€‚

## 2.softmax
### 2.1åŸºæœ¬æ¦‚å¿µ
ä¸»è¦ç”¨æ¥å¤„ç†ç¦»æ•£çš„åˆ†ç±»é—®é¢˜ã€‚ï¼ˆps ç¦»æ•£é—®é¢˜å’Œè¿ç»­é—®é¢˜å·®å¼‚ï¼šæ¯”å¦‚è¯´é¢„æµ‹æˆ¿ä»·æˆ¿ä»·æ˜¯ä¸€ä¸ªè¿ç»­çš„æ•°ã€‚æ‰€è°“çš„ç¦»æ•£é—®é¢˜å°±æ˜¯æ¯”å¦‚è¯´åˆ¤æ–­æ˜¯å¦å±äºè¿™ä¸€ç±»ã€‚ï¼‰

ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥å›å½’æ˜¯ä¸€ä¸ªå•å±‚ç¥ç»ç½‘ç»œã€‚

softmaxå…¬å¼å…·ä½“[é“¾æ¥](https://baike.baidu.com/item/softmax%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/22689563?fr=aladdin)å¦‚ä¸‹
æ³¨æ„çš„æ˜¯softmaxä¸­bå‚æ•°å–å†³äºåˆ†ç±»çš„ç§ç±»æ•°é‡ï¼Œæ¯”å¦‚åˆ†ä¸‰ç±»å°±æœ‰ä¸‰ä¸ªï¼Œå’Œå‰é¢çš„çº¿æ€§å›å½’æœ‰äº›ä¸åŒã€‚

## 3.å¤šå±‚æ„ŸçŸ¥æœº
### 3.1 æ¦‚å¿µ
ç®€å•çš„ç†è§£å°±æ˜¯æ¨¡å‹çš„å±‚æ•°ä¾¿å¤šäº†ã€‚å¹¶ä¸”åµŒå…¥äº†æ¿€æ´»å‡½æ•°ã€‚
### 3.2 æ¿€æ´»å‡½æ•°
å¼•å…¥æ¿€æ´»å‡½æ•°çš„åŸå› ï¼šä¸€ç›´ä½¿ç”¨åµŒå¥—çš„çº¿æ€§å‡½æ•°è®­ç»ƒå‡ºçš„æ•ˆæœå’Œå•ä¸€æ›¾çš„çº¿æ€§æ¨¡å‹æ•ˆæœå·®ä¸å¤š

æ¿€æ´»å‡½æ•°æœ‰å¾ˆå¤šç§å¦‚reluå¸¸ç”¨çš„ï¼Œsigmoidï¼Œè¿˜æœ‰tanh
å¦‚ä½•é€‰æ‹©æ¿€æ´»å‡½æ•°ï¼šåœ¨ä¸­é—´å±‚å¯ä»¥ä½¿ç”¨reluç­‰(è®¡ç®—å¿«ï¼Œè€Œä¸”æ˜¯ä¸€ä¸ªå¤§çš„åŒºé—´ï¼Œä¸ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜)ã€‚åœ¨è¾“å‡ºå±‚å¯ä»¥ä½¿ç”¨sigmoidæ¥å°†å€¼è½¬åŒ–åˆ°0-1ä¹‹é—´ã€‚
